{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 15:33:17.439481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image =cv2.cvtColor(cv2.imread('img.jpg'),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "test_image = cv2.resize(test_image, (256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 15:33:19.518656: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "patches = tf.image.extract_patches(images=tf.expand_dims(test_image, axis = 0),\n",
    "                                    sizes=[1, 16, 16, 1],\n",
    "                                    strides = [1, 16, 16, 1],\n",
    "                                    rates = [1, 1, 1, 1],\n",
    "                                    padding = 'VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 256 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "k=0\n",
    "for i in range(16):\n",
    "    for j in range(16):\n",
    "        \n",
    "        ax = plt.subplot(16,16,k+1)\n",
    "        plt.imshow(tf.reshape(patches[0,i,j,:], (16,16,3)))\n",
    "        plt.axis('off')\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
    "        super(PatchEncoder, self).__init__(name='patch_encoder')\n",
    "\n",
    "\n",
    "        self.linear_projection = Dense(HIDDEN_SIZE)\n",
    "        self.positional_emb = Embedding(N_PATCHES, HIDDEN_SIZE)\n",
    "        self.N_PATCHES = N_PATCHES\n",
    "        \n",
    "    def call(self, x):\n",
    "        patches = tf.image.extract_patches(images=x,\n",
    "                                sizes=[1, 16, 16, 1],\n",
    "                                strides = [1, 16, 16, 1],\n",
    "                                rates = [1, 1, 1, 1],\n",
    "                                padding = 'VALID')\n",
    "        \n",
    "        patches = tf.reshape(patches, (patches.shape[0], -1, 768))\n",
    "        embedding_input = tf.range(start = 0, limit = self.N_PATCHES, delta = 1)\n",
    "        output = self.linear_projection(patches) + self.positional_emb(embedding_input)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256, 768), dtype=float32, numpy=\n",
       "array([[[ 0.04225496, -0.0304908 ,  0.03246274, ...,  0.04103538,\n",
       "          0.00375139,  0.02053   ],\n",
       "        [-0.0350764 , -0.00732142, -0.03070921, ...,  0.03638751,\n",
       "         -0.0136521 , -0.00281046],\n",
       "        [ 0.03411654,  0.03561426, -0.02416507, ..., -0.02547295,\n",
       "         -0.0007113 , -0.04138904],\n",
       "        ...,\n",
       "        [ 0.00130989, -0.04318024,  0.01787912, ..., -0.03103855,\n",
       "          0.0393716 ,  0.04882506],\n",
       "        [ 0.02534206,  0.01186565, -0.01764246, ..., -0.00965464,\n",
       "         -0.00793453, -0.0011136 ],\n",
       "        [ 0.02479683, -0.04845394, -0.03232636, ...,  0.00411453,\n",
       "          0.03435694,  0.01665336]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_enc = PatchEncoder(256, 768)\n",
    "patch_enc(tf.zeros([1,256,256,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, N_HEADS, HIDDEN_SIZE):\n",
    "        super(TransformerEncoder, self).__init__(name = 'transformer_encoder')\n",
    "\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "\n",
    "\n",
    "        self.multi_head_att = MultiHeadAttention(N_HEADS, HIDDEN_SIZE)\n",
    "        self.dense_1 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
    "        self.dense_2 = Dense(HIDDEN_SIZE, activation = tf.nn.gelu)\n",
    "\n",
    "    def call(self,input):\n",
    "\n",
    "        x_1 = self.layer_norm1(input)\n",
    "        x_1 = self.multi_head_att(x_1,x_1)\n",
    "\n",
    "        x_1 = Add()([x_1,input])\n",
    "\n",
    "        x_2 = self.layer_norm2(x_1)\n",
    "        x_2 = self.dense_1(x_2)\n",
    "        output = self.dense_2(x_2)\n",
    "        output = Add()([output,x_1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256, 768), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_enc = TransformerEncoder(8,768)\n",
    "trans_enc(tf.zeros([1,256,768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "class Vit(Model):\n",
    "    def __init__(self,N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNITS, NUM_CLASSES):\n",
    "        super(Vit,self).__init__(name = 'vision_transformer')\n",
    "        \n",
    "        self.N_LAYERS = N_LAYERS\n",
    "        self.patch_encoder = PatchEncoder(N_PATCHES,HIDDEN_SIZE)\n",
    "        self.transf_encoder = [TransformerEncoder(N_HEADS,HIDDEN_SIZE) for _ in range(N_LAYERS)]\n",
    "        self.dense_1 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
    "        self.dense_2 = Dense(N_DENSE_UNITS, tf.nn.gelu)\n",
    "        self.dense_3 = Dense(NUM_CLASSES, activation = 'softmax')\n",
    "    def call(self,input, training = True):\n",
    "\n",
    "        x = self.patch_encoder(input)\n",
    "\n",
    "\n",
    "        for i in range(self.N_LAYERS):\n",
    "            x = self.transf_encoder[i](x)\n",
    "        \n",
    "        x = Flatten()(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        output = self.dense_3(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 256, 256, 3)\n",
    "vit = Vit(N_HEADS=8, HIDDEN_SIZE=768, N_PATCHES=256, N_LAYERS=6, N_DENSE_UNITS=128, NUM_CLASSES=10)\n",
    "\n",
    "vit.build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " patch_encoder (PatchEncoder  multiple                 787200    \n",
      " )                                                               \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  multiple                 20077824  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " dense_123 (Dense)           multiple                  25165952  \n",
      "                                                                 \n",
      " dense_124 (Dense)           multiple                  16512     \n",
      "                                                                 \n",
      " dense_125 (Dense)           multiple                  1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,437,898\n",
      "Trainable params: 146,437,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " patch_encoder (PatchEncoder  (None, 256, 768)         787200    \n",
      " )                                                               \n",
      "                                                                 \n",
      " transformer_encoder0 (Trans  (None, 256, 768)         20077824  \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_encoder1 (Trans  (None, 256, 768)         20077824  \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 196608)            0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 128)               25165952  \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,126,602\n",
      "Trainable params: 66,126,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Embedding, LayerNormalization, MultiHeadAttention, Add, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
    "        super(PatchEncoder, self).__init__(name='patch_encoder')\n",
    "        self.linear_projection = Dense(HIDDEN_SIZE)\n",
    "        self.positional_emb = Embedding(N_PATCHES, HIDDEN_SIZE)\n",
    "        self.N_PATCHES = N_PATCHES\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        patches = tf.image.extract_patches(images=x,\n",
    "                                           sizes=[1, 16, 16, 1],\n",
    "                                           strides=[1, 16, 16, 1],\n",
    "                                           rates=[1, 1, 1, 1],\n",
    "                                           padding='VALID')\n",
    "        patches = tf.reshape(patches, (batch_size, -1, patches.shape[-1]))\n",
    "        embedding_input = tf.range(start=0, limit=self.N_PATCHES, delta=1)\n",
    "        output = self.linear_projection(patches) + self.positional_emb(embedding_input)\n",
    "        return output\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, N_HEADS, HIDDEN_SIZE,i):\n",
    "        super(TransformerEncoder, self).__init__(name='transformer_encoder'+str(i))\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.multi_head_att = MultiHeadAttention(N_HEADS, HIDDEN_SIZE)\n",
    "        self.dense_1 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n",
    "        self.dense_2 = Dense(HIDDEN_SIZE, activation=tf.nn.gelu)\n",
    "\n",
    "    def call(self, input):\n",
    "        x_1 = self.layer_norm1(input)\n",
    "        x_1 = self.multi_head_att(x_1, x_1)\n",
    "        x_1 = Add()([x_1, input])\n",
    "        x_2 = self.layer_norm2(x_1)\n",
    "        x_2 = self.dense_1(x_2)\n",
    "        output = self.dense_2(x_2)\n",
    "        output = Add()([output, x_1])\n",
    "        return output\n",
    "\n",
    "def create_vit_model(input_shape, N_HEADS, HIDDEN_SIZE, N_PATCHES, N_LAYERS, N_DENSE_UNITS, NUM_CLASSES):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = PatchEncoder(N_PATCHES, HIDDEN_SIZE)(inputs)\n",
    "    for i in range(N_LAYERS):\n",
    "        x = TransformerEncoder(N_HEADS, HIDDEN_SIZE,i)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(N_DENSE_UNITS, activation=tf.nn.gelu)(x)\n",
    "    x = Dense(N_DENSE_UNITS, activation=tf.nn.gelu)(x)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "vit = create_vit_model(input_shape, N_HEADS=8, HIDDEN_SIZE=768, N_PATCHES=256, N_LAYERS=2, N_DENSE_UNITS=128, NUM_CLASSES=10)\n",
    "vit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marcelo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
